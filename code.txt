File Name : losses/losses.py
Code: 
from tinygrad_clone.tensor import Value

def nll_loss(probs, target_index):
    """
    Calculates the Negative Log Likelihood loss.

    Args:
        probs: A list of Value objects representing softmax probabilities.
        target_index: The index of the correct class (integer).

    Returns:
        A Value object representing the NLL loss.
    """
    log_prob = probs[target_index].log()  # Log probability of the target class
    return -log_prob  # Negative log likelihood

def mse_loss(outputs, targets):
    """
    Mean Squared Error Loss

    Args:
        outputs: A list of Value objects (predictions).
        targets: A list of Value objects (actual values).

    Returns:
        A Value object representing the MSE loss.
    """
    loss = sum((o - t) ** 2 for o, t in zip(outputs, targets)) / len(outputs)
    return loss


----------------------------------------------------------------------------------------

File Name : tt/__init__.py
Code: 

# tinygrad_clone/nn/__init__.py
from .neuron import Neuron
from .layer import Layer
from .mlp import MLP
from .activations import SoftmaxLayer
from .loss import CrossEntropyLoss
__all__ = ["Neuron", "Layer", "MLP", "SoftmaxLayer","CrossEntropyLoss"]

----------------------------------------------------------------------------------------

File Name : tt/activations.py
Code: 

from tinygrad_clone.tensor import Value

class SoftmaxLayer:
    def __init__(self):
        pass


    def __call__(self, logits):
        # Stabilize softmax by subtracting the max logit
        
        max_logit = max(logit.data for logit in logits)  # Find the maximum logit value
        counts = [(logit - max_logit).exp() for logit in logits]  # Subtract max_logit and compute exp

        denominator = sum(counts)
        self.probs = [c / denominator for c in counts]  # Normalize to get probabilities
        return self.probs

----------------------------------------------------------------------------------------

File Name : tt/layer.py
Code: 

from tinygrad_clone.tt.neuron import Neuron

class Layer:
    def __init__(self, nin, nout):
        self.neurons = [Neuron(nin) for _ in range(nout)]

    def __call__(self, x):
        outs = [n(x) for n in self.neurons]
        return outs[0] if len(outs) == 1 else outs

    def parameters(self):
        return [p for neuron in self.neurons for p in neuron.parameters()]

----------------------------------------------------------------------------------------

File Name : tt/mlp.py
Code: 

from tinygrad_clone.tt.layer import Layer
from tinygrad_clone.tt.activations import SoftmaxLayer

class MLP:
    def __init__(self, nin, nouts):
        sz = [nin] + nouts
        self.layers = [Layer(sz[i], sz[i + 1]) for i in range(len(nouts))]
        self.softmax = SoftmaxLayer()

    def __call__(self, x):
        for layer in self.layers:
            x = layer(x)
        self.probs = self.softmax(x)
        return self.probs



    def parameters(self):
        return [p for layer in self.layers for p in layer.parameters()]

    def zero_grad(self):
        for p in self.parameters():
            p.grad = 0.0

----------------------------------------------------------------------------------------

File Name : tt/neuron.py
Code:

from tinygrad_clone.tensor import Value
import random

class Neuron:
    def __init__(self, nin):
        self.w = [Value(random.uniform(-1, 1)) for _ in range(nin)]
        self.b = Value(random.uniform(-1, 1))

    def __call__(self, x):
        act = sum((wi * xi for wi, xi in zip(self.w, x)), self.b)
        out = act.leaky_relu()
        return out

    def parameters(self):
        return self.w + [self.b]


----------------------------------------------------------------------------------------

File Name : tt/optimizer.py
Code:

from tinygrad_clone.tensor import Value

class Adam:
    def __init__(self, parameters, lr=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):
        self.parameters = parameters
        self.lr = lr
        self.beta1 = beta1
        self.beta2 = beta2
        self.epsilon = epsilon
        self.m = [Value(0.0) for _ in parameters]  # Initialize first moment vector
        self.v = [Value(0.0) for _ in parameters]  # Initialize second moment vector
        self.t = 0  # Initialize time step

    def step(self):
        self.t += 1
        for i, p in enumerate(self.parameters):
            # Update biased first moment estimate
            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * p.grad
            # Update biased second moment estimate
            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * (p.grad * p.grad)

            # Compute bias-corrected first moment estimate
            m_hat = self.m[i] / (1 - self.beta1 ** self.t)
            # Compute bias-corrected second moment estimate
            v_hat = self.v[i] / (1 - self.beta2 ** self.t)
            #print("Adam Sqrt",v_hat**0.5,self.epsilon)
            # Update parameters
            p.data -= self.lr * m_hat.data / ((v_hat**0.5).data + self.epsilon)

----------------------------------------------------------------------------------------

File Name : tt/dataset.py
Code:


import pandas as pd
import numpy as np
from tinygrad_clone.tensor import Value
from sklearn.model_selection import train_test_split

class Dataset:
    def __init__(self, file_path, target_column, drop_columns=[]):
        self.file_path = file_path
        self.target_column = target_column
        self.drop_columns = drop_columns
        self.X_train, self.X_test, self.y_train, self.y_test = self._load_and_preprocess()

    def _load_and_preprocess(self):
        df = pd.read_csv(self.file_path)

        # Drop unnecessary columns
        df = df.drop(columns=self.drop_columns, errors="ignore")

        # Handle missing values
        df = df.dropna()

        # Convert target variable to numerical encoding if needed
        if df[self.target_column].dtype == 'object':
            df[self.target_column] = pd.factorize(df[self.target_column])[0]

        # Extract features and target
        X = df.drop(columns=[self.target_column]).values
        y = df[[self.target_column]].values

        # Normalize features
        X = (X - X.mean(axis=0)) / X.std(axis=0)

        # Split into training and testing sets
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

        return X_train, X_test, y_train, y_test

    def get_data(self):
        # Return the training and test sets
        return self.X_train, self.X_test, self.y_train, self.y_test


----------------------------------------------------------------------------------------

File Name : tt/tensor.py
Code:

import math
import numpy as np
import random

class Value:
    def __init__(self, data, _children=(), _op='', label=''):
        self.data = data
        self.grad = 0.0
        self._backward = lambda: None
        self._prev = set(_children)
        self._op = _op
        self.label = label

    def __repr__(self):
        return f"Value(data={self.data})"

    def __add__(self, other):
        other = other if isinstance(other, Value) else Value(other)
        out = Value(self.data + other.data, (self, other), '+')

        def _backward():

            self.grad += 1.0 * out.grad
            other.grad += 1.0 * out.grad
        out._backward = _backward

        return out

    def __mul__(self, other):
        other = other if isinstance(other, Value) else Value(other)
        out = Value(self.data * other.data, (self, other), '*')

        def _backward():

            self.grad += other.data * out.grad
            other.grad += self.data * out.grad
        out._backward = _backward

        return out

    def __pow__(self, other):
        assert isinstance(other, (int, float)), "only supporting int/float powers for now"
        out = Value(self.data**other, (self,), f'**{other}')

        def _backward():

            self.grad += other * (self.data ** (other - 1)) * out.grad
        out._backward = _backward

        return out

    def __rmul__(self, other):
        return self * other

    def __truediv__(self, other):
        return self * other**-1

    def __neg__(self):
        return self * -1

    def __sub__(self, other):
        return self + (-other)

    def __radd__(self, other):
        return self + other

    def tanh(self):
        x = self.data
        t = math.tanh(x)
        out = Value(t, (self,), 'tanh')

        def _backward():

            self.grad += (1 - t**2) * out.grad
        out._backward = _backward

        return out

    def exp(self):
        x = self.data
        out = Value(math.exp(x), (self,), 'exp')

        def _backward():

            self.grad += out.data * out.grad
        out._backward = _backward

        return out

    def log(self):
        x = max(self.data, 1e-9)
        out = Value(math.log(x), (self,), 'log')

        def _backward():

            self.grad += (1 / x) * out.grad
        out._backward = _backward

        return out

    def leaky_relu(self, alpha=0.01):
        out = Value(self.data if self.data > 0 else self.data * alpha, (self,), 'leaky_relu')

        def _backward():
 
            self.grad += (1 if self.data > 0 else alpha) * out.grad
        out._backward = _backward

        return out

    def backward(self):
        topo = []
        visited = set()

        def build_topo(v):
            if v not in visited:
                visited.add(v)
                for child in v._prev:
                    build_topo(child)
                topo.append(v)

        build_topo(self)

        self.grad = 1.0
        for node in reversed(topo):
            node._backward()


----------------------------------------------------------------------------------------

File Name : tt/train_test1.py
Code:


import pickle
from tinygrad_clone.dataset import Dataset
from tinygrad_clone.tt.mlp import MLP
from tinygrad_clone.losses.losses import nll_loss, mse_loss
from tinygrad_clone.tensor import Value  # Import the Value class
from tinygrad_clone.tt.optimizer import Adam
import numpy as np

import dill as pickle
# Load dataset
dataset = Dataset(file_path="penguins.csv", target_column="species", drop_columns=['island', 'sex'])
X_train, X_test, y_train, y_test = dataset.get_data()
#print("X train",X_train)
# Initialize MLP
mlp = MLP(nin=len(X_train[0]), nouts=[6, 3])

# Set hyperparameters
lr = 0.01
epochs = 200
loss_function = "cross_entropy"  # Choose between "cross_entropy" or "mse"
training_accuracies = []

adam_optimizer = Adam(mlp.parameters(), lr=0.001)
# Training loop
for epoch in range(epochs):
    total_loss = 0
    correct = 0  # Track correct predictions for training accuracy
    for x_np, target_idx in zip(X_train, y_train):  # Iterate using X_train and y_train
        #print(x_np)
        x = [Value(xi) for xi in x_np]  # Convert x to Value objects
        output_probs = mlp(x)  # Forward pass

        # Compute loss
        if loss_function == "cross_entropy":
            loss = nll_loss(output_probs, target_idx.item())  # Cross-Entropy Loss
        elif loss_function == "mse":
            loss = mse_loss(output_probs, [Value(target_idx)])  # MSE Loss
        else:
            raise ValueError("Invalid loss function selected.")

        loss.backward()  # Backward pass
        total_loss += loss.data  # Accumulate total loss

        adam_optimizer.step()
        # Track correct predictions
        predicted_idx = np.argmax([p.data for p in output_probs])  # Get the index of the predicted class
        if predicted_idx == target_idx:
            correct += 1  # Increment correct count if prediction matches target
        mlp.zero_grad()  # Reset gradients
    # Compute training accuracy for this epoch
    train_accuracy = correct / len(y_train) * 100   
    training_accuracies.append(train_accuracy)

    avg_loss = total_loss / len(X_train)  # Calculate average loss
    print(f"Epoch {epoch+1}: Loss {avg_loss:.4f}, Training Accuracy: {train_accuracy:.2f}%")

print(f"Avg Train Accuracy: {np.mean(training_accuracies):.2f}%")

# After training, save the model
with open('model.pkl', 'wb') as f:
    pickle.dump(mlp, f)  # Save the trained model

print("Model saved successfully.")

# Evaluate the model
correct = 0
total = len(X_test)




from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Initialize lists to store true labels and predicted labels
y_true = []
y_pred = []

# Evaluate the model on the test set
for x, y in zip(X_test, y_test):
    out = mlp(x)
    predicted_class = out.index(max(out, key=lambda v: v.data))  # Get index of max probability
    actual_class = int(y.item())
    if predicted_class == actual_class:
        correct += 1
    y_true.append(actual_class)
    y_pred.append(predicted_class)
    
accuracy = correct / total * 100
print(f"Test Accuracy: {accuracy:.2f}%")
# Calculate confusion matrix
cm = confusion_matrix(y_true, y_pred)

# Display confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=range(len(set(y_true))))
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.show()

# Print classification report
print("Classification Report:")
print(classification_report(y_true, y_pred))


